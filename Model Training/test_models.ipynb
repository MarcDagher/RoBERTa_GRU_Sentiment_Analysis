{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, Stemmer, Lemmatizer, and Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean text and split by words\n",
    "import re\n",
    "\n",
    "def clean_text_for_LR(text, stopwords = stopwords, stemmer = stemmer, lemmatizer = lemmatizer):\n",
    "    text = re.sub(r'<.*?>', '', text) # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text) # Remove non-letter characters\n",
    "    text = text.lower() # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra white spaces\n",
    "    # Split words then lemmatize and stem words then remove stopwords\n",
    "    words = re.split(r'\\W+', text)\n",
    "    # words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stopwords] \n",
    "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stopwords] \n",
    "    \n",
    "    # return list of strings\n",
    "    words = \" \".join(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocab: 70709\n",
      "1_2_ngram Vocab: 2641907\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load BoW_vectorizer\n",
    "with open('../Models/BoW_logistic_regression/BoW_vectorizer.pkl', 'rb') as f:\n",
    "    BoW_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load n_gram_1_2_vectorizer\n",
    "with open('../Models/1_2_gram_logistic_regression/1_2_gram_vectorizer.pkl', 'rb') as f:\n",
    "    n_gram_1_2_vectorizer = pickle.load(f)\n",
    "\n",
    "print(f\"BoW Vocab: {len(BoW_vectorizer.vocabulary_)}\")\n",
    "print(f\"1_2_ngram Vocab: {len(n_gram_1_2_vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the class of the review\n",
    "def predict_with_LR(text, vectorizer, model):\n",
    "  text = clean_text_for_LR(text)\n",
    "  text = vectorizer.transform([text]).toarray()\n",
    "  \n",
    "  # Predict using model\n",
    "  prediction = model.predict(text) # Predict\n",
    "  prediction = prediction[0] # Adjust prediction format to print it\n",
    "  return prediction\n",
    "\n",
    "# Function to compare the model's answer with the manual reviews answer\n",
    "def results_on_manual_reviews(index, prediction, answers, model_name):\n",
    "  if prediction == 1.0: print(f\"{model_name} Prediction: {prediction} => Positive Sentiment => {prediction == answers[index]}\")\n",
    "  else: print(f\"{model_name} Prediction: {prediction} => Negative Sentiment => {prediction == answers[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample reviews for testing the models\n",
    "phrases = [\"The movie had an excellent storyline with amazing acting, but the ending was quite disappointing and left me unsatisfied.\", ## Negative\n",
    "\n",
    "          \"This movie was absolutely fantastic! The story was gripping, the acting was top-notch, and the cinematography was stunning. I was on the edge of my seat the entire time and couldn't look away. It's been a long time since a film has captivated me like this. Highly recommend!\", ## Positive \n",
    "\n",
    "          \"The film had its moments, but overall, it failed to leave a lasting impression. The characters lacked depth, and the storyline felt disjointed. It's a forgettable movie that I wouldn't recommend to others.\", # neutral - to - negative\n",
    "          \n",
    "          \"I hate this movie\", # Negative\n",
    "\n",
    "          \"I have a love hate relationship with this movie\", # Negative\n",
    "\n",
    "          \"I wish I could watch it again now\", # Positive\n",
    "\n",
    "          \"I love this movie\", # Positive\n",
    "\n",
    "          \"Great\" # Positive\n",
    "          ] \n",
    "\n",
    "answers = [0, 1, 0, 0, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR BoW Prediction: 0 => Negative Sentiment => True\n",
      "LR BoW Prediction: 1 => Positive Sentiment => True\n",
      "LR BoW Prediction: 0 => Negative Sentiment => True\n",
      "LR BoW Prediction: 1 => Positive Sentiment => False\n",
      "LR BoW Prediction: 1 => Positive Sentiment => False\n",
      "LR BoW Prediction: 1 => Positive Sentiment => True\n",
      "LR BoW Prediction: 1 => Positive Sentiment => True\n",
      "LR BoW Prediction: 1 => Positive Sentiment => True\n",
      "\n",
      "\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => False\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => True\n",
      "LR 1_2_n_gram Prediction: 0 => Negative Sentiment => True\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => False\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => False\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => True\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => True\n",
      "LR 1_2_n_gram Prediction: 1 => Positive Sentiment => True\n"
     ]
    }
   ],
   "source": [
    "## Load SGDClassifiers (Logistic Regression)\n",
    "from joblib import load\n",
    "\n",
    "# Logistic regression trained on BoW\n",
    "LR_BoW_model = load('../Models/BoW_logistic_regression/BoW_Logistic_Regression.pkl')\n",
    "\n",
    "# Logistic regression trained on 1_2 ngrams\n",
    "LR_1_2_gram_model = load('../Models/1_2_gram_logistic_regression/1_2_gram_Logistic_Regression.pkl')\n",
    "\n",
    "# Test models on manual reviews\n",
    "for i in range(len(phrases)):\n",
    "  prediction = predict_with_LR(model=LR_BoW_model, text=phrases[i], vectorizer=BoW_vectorizer)\n",
    "  results_on_manual_reviews(prediction=prediction, model_name=\"LR BoW\", index=i, answers=answers)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(phrases)):\n",
    "  prediction = predict_with_LR(model=LR_1_2_gram_model, text=phrases[i], vectorizer=n_gram_1_2_vectorizer)\n",
    "  results_on_manual_reviews(prediction=prediction, model_name=\"LR 1_2_n_gram\", index=i, answers=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RoBERTa GRU model architecture\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "class RoBERTa_GRU(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RoBERTa_GRU, self).__init__()\n",
    "\n",
    "    # Initialize the layers that are needed\n",
    "    self.RoBERTa = RobertaModel.from_pretrained('roberta-base')\n",
    "    self.gru = torch.nn.GRU( input_size = 768, hidden_size = 256 )\n",
    "    self.flatten = torch.nn.Flatten()\n",
    "    self.dense_1 = torch.nn.Linear( in_features = 256, out_features=1000 )\n",
    "    self.gelu = torch.nn.GELU()\n",
    "    self.dense_2 = torch.nn.Linear( in_features=1000, out_features=2 )\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "    # Get the last_hidden_states from RoBERTa\n",
    "    roberta_output = self.RoBERTa(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    # Select the first column from RoBERTa's last_hidden_state output. CLS is the classification of each sequence\n",
    "    cls = roberta_output.last_hidden_state[:, 0]\n",
    "\n",
    "    # GRU will learn the longterm dependencies of the sequences\n",
    "    sequences, gru_hidden_states = self.gru( cls )\n",
    "\n",
    "    # Doesn't make any difference\n",
    "    flattened = self.flatten( sequences )\n",
    "\n",
    "    # Learn relationships between the hidden states\n",
    "    x = self.dense_1( flattened )\n",
    "    x = self.gelu(x)\n",
    "\n",
    "    # Return the prediction\n",
    "    x = self.dense_2(x)\n",
    "    output = torch.nn.functional.softmax(x, dim=1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RoBERTa_GRU(\n",
       "  (RoBERTa): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (gru): GRU(768, 256)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dense_1): Linear(in_features=256, out_features=1000, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       "  (dense_2): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"../../RoBERTa_GRU Model [Not in GitHub]/RoBERTa_Pretrained_Tokenizer\")\n",
    "\n",
    "# Load pretrained RoBERTa_GRU into the model architecture\n",
    "model = RoBERTa_GRU()\n",
    "model = torch.load('../../RoBERTa_GRU Model [Not in GitHub]/RoBERTa_GRU_model.pth', map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare review for prediction\n",
    "def preprocess_for_RoBERTa_GRU(text, tokenizer, torch):\n",
    "  \n",
    "  # Tokenize review\n",
    "  inputs = tokenizer.encode_plus(text, None,\n",
    "                                 truncation = True, padding = 'max_length', max_length = 512, \n",
    "                                 add_special_tokens = True, return_token_type_ids = True)\n",
    "\n",
    "  # Prepare inputs for the RoBERTA_GRU model\n",
    "  ids = torch.tensor(inputs['input_ids'], dtype=torch.long).unsqueeze(0)\n",
    "  attention_mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).unsqueeze(0)\n",
    "  token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "  return ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "# Return classification/sentiment of a single review\n",
    "def predict_using_RoBERTa_GRU(model, text, tokenizer, torch):\n",
    "  ids, attention_mask, token_type_ids = preprocess_for_RoBERTa_GRU(text, tokenizer, torch)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    y_pred = model.forward(ids, attention_mask, token_type_ids)\n",
    "\n",
    "    return torch.argmax(y_pred).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa_GRU Prediction: 0 => Negative Sentiment => True\n",
      "RoBERTa_GRU Prediction: 1 => Positive Sentiment => True\n",
      "RoBERTa_GRU Prediction: 0 => Negative Sentiment => True\n",
      "RoBERTa_GRU Prediction: 0 => Negative Sentiment => True\n",
      "RoBERTa_GRU Prediction: 0 => Negative Sentiment => True\n",
      "RoBERTa_GRU Prediction: 1 => Positive Sentiment => True\n",
      "RoBERTa_GRU Prediction: 1 => Positive Sentiment => True\n",
      "RoBERTa_GRU Prediction: 1 => Positive Sentiment => True\n"
     ]
    }
   ],
   "source": [
    "# Text RoBERTa on manual reviews\n",
    "for i in range(len(phrases)):\n",
    "  prediction = predict_using_RoBERTa_GRU(model, phrases[i], tokenizer, torch)\n",
    "\n",
    "  if prediction == 1: print(f\"RoBERTa_GRU Prediction: {prediction} => Positive Sentiment => {prediction == answers[i]}\")\n",
    "  else: print(f\"RoBERTa_GRU Prediction: {prediction} => Negative Sentiment => {prediction == answers[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
